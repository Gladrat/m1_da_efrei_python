## üéØ Objectif

Votre mission est de concevoir un **rapport de benchmark** √©valuant les performances d‚Äôun ou plusieurs mod√®les d‚ÄôIA sur des questions de culture g√©n√©rale.

L‚Äôexercice permet de mettre en ≈ìuvre un **pipeline complet** de data ing√©nierie :

- collecte et int√©gration de donn√©es,
- enrichissement par appel √† des mod√®les d‚ÄôIA,
- g√©n√©ration et visualisation de r√©sultats.

## üóÇÔ∏è √âtape 1 : Constitution d‚Äôun dataset de questions de culture g√©n√©rale

1. **Source de donn√©es** :
    - Utilisez le site [Open Trivia Database (OpenTDB)](https://opentdb.com/), une API publique proposant des milliers de questions cat√©goris√©es (science, histoire, divertissement, etc.).
2. **Format du dataset brut** :
    - Conservez les colonnes suivantes :
        - `question` : l‚Äôintitul√© de la question
        - `category` : la cat√©gorie (ex : Histoire, Sciences, etc.)
        - `difficulty` : niveau de difficult√© (facile, moyen, difficile)
        - `correct_answer` : la bonne r√©ponse
        - `incorrect_answers` : les distracteurs propos√©s
        - `type` : le type de question (multiple ou bool√©en)
3. **Stockage initial** :
    - Sauvegardez vos donn√©es dans un fichier **CSV** (`questions_raw.csv`).

## üîÑ √âtape 2 : Module de data int√©gration & enrichissement avec un mod√®le d‚ÄôIA

1. **Pr√©traitement** :
    - Nettoyez les donn√©es si n√©cessaire.
    - Uniformisez la casse et le formatage des r√©ponses si n√©cessaire.
2. **Int√©gration de l‚ÄôIA avec Ollama** :
    - Installez [Ollama](https://ollama.ai/) (un runtime l√©ger permettant d‚Äôex√©cuter localement des mod√®les LLM).
    - T√©l√©chargez un mod√®le adapt√© √† votre machine (par exemple `llama3.2`, `gemma3`, ou tout autre mod√®le support√©).
    - Utilisez l‚Äô**API Python d‚ÄôOllama** pour automatiser la g√©n√©ration de r√©ponses IA :
    - Pour chaque question de votre dataset, interrogez le mod√®le et stockez la r√©ponse g√©n√©r√©e.
3. **Enrichissement du dataset** :
    - Ajoutez les colonnes suivantes :
        - `ai_answer` : r√©ponse donn√©e par le mod√®le
        - `ai_correct` : bool√©en (`True` si la r√©ponse IA correspond √† la bonne r√©ponse)
        - `response_time` : temps de g√©n√©ration mesur√© en secondes
    - Exportez le dataset enrichi dans un fichier `benchmark_results.csv`.

## ‚ö†Ô∏è Importance du prompt

Le **prompt** (la fa√ßon dont vous posez la question au mod√®le) influence fortement la qualit√© des r√©ponses obtenues.

- Une m√™me question peut donner des r√©sultats tr√®s diff√©rents selon le contexte fourni.
- Exemple :
    - Prompt simple : *"Qui a peint la Joconde ?"*
    - Prompt plus robuste : *"R√©ponds uniquement par un nom propre. Question : Qui a peint la Joconde ?"*
    - Il existe des versions encore plus optimis√©es, √† vous de les trouver !
- Dans ce cas pratique, il est recommand√© de **standardiser les prompts** pour que le benchmark soit coh√©rent.
- Vous pouvez tester plusieurs variantes de prompt afin d‚Äôobserver leur impact sur :
    - le **taux de bonnes r√©ponses**,
    - la **longueur et pr√©cision** des r√©ponses,
    - la **robustesse** face aux ambigu√Øt√©s.

Pour un benchmark rigoureux, vous pouvez garder une trace de la formulation du prompt dans votre dataset (`prompt_version`).

## üìä √âtape 3 : Rapport de benchmark

Vous allez produire un **rapport d‚Äôanalyse des r√©sultats** afin de comparer les performances du mod√®le.

### Analyses possibles

- **Performance globale** : taux de bonnes r√©ponses (%)
- **Par cat√©gorie** : pr√©cision par th√®me (histoire, sciences, etc.)
- **Par niveau de difficult√©** : facile vs difficile
- **Par temps de r√©ponse** : rapidit√© moyenne par question
- **Comparaison de mod√®les** (optionnel) : ex√©cutez le benchmark avec plusieurs mod√®les disponibles via Ollama (`llama2`, `mistral`, etc.) et comparez leurs performances.

### M√©thodes de restitution

1. **Notebook** :
    - Utilisez **Pandas** pour manipuler les donn√©es.
    - Utilisez **Matplotlib** ou **Seaborn** pour cr√©er des visualisations :
        - histogrammes de pr√©cision,
        - courbes de temps de r√©ponse,
        - heatmaps par cat√©gorie et difficult√©.
2. **Application interactive** :
    - Vous pouvez aller plus loin en utilisant **Streamlit** :
        - Chargement dynamique du dataset enrichi,
        - Tableaux filtrables,
        - Graphiques interactifs,
        - Comparaison entre plusieurs mod√®les.

## üì¶ Livrables attendus

Cr√©ez un d√©p√¥t Github contenant :

0. **Script de scraping**
1. **Dataset brut** (`questions_raw.csv`)
2. **Dataset enrichi** (`benchmark_results.csv`)
3. **Notebook d‚Äôanalyse** (`benchmark_analysis.ipynb`)
OU
3. **Application Streamlit/Dash** pour un rapport interactif